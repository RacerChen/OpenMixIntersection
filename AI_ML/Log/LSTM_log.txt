lr: 0.005, hidden_size: 32, layer_num: 1, train_batch_size: 4 - min val loss: 1.2148799697558086
lr: 0.005, hidden_size: 32, layer_num: 1, train_batch_size: 8 - min val loss: 1.1576735178629558
lr: 0.005, hidden_size: 32, layer_num: 1, train_batch_size: 16 - min val loss: 1.118367314338684
lr: 0.005, hidden_size: 32, layer_num: 1, train_batch_size: 32 - min val loss: 1.1283902525901794
lr: 0.005, hidden_size: 32, layer_num: 2, train_batch_size: 4 - min val loss: 1.1381370822588603
lr: 0.005, hidden_size: 32, layer_num: 2, train_batch_size: 8 - min val loss: 1.086265762646993
lr: 0.005, hidden_size: 32, layer_num: 2, train_batch_size: 16 - min val loss: 0.9746739367643992
lr: 0.005, hidden_size: 32, layer_num: 2, train_batch_size: 32 - min val loss: 1.0129545331001282
lr: 0.005, hidden_size: 32, layer_num: 3, train_batch_size: 4 - min val loss: 1.1376234889030457
lr: 0.005, hidden_size: 32, layer_num: 3, train_batch_size: 8 - min val loss: 1.1093119978904724
lr: 0.005, hidden_size: 32, layer_num: 3, train_batch_size: 16 - min val loss: 1.0192461212476094
lr: 0.005, hidden_size: 32, layer_num: 3, train_batch_size: 32 - min val loss: 1.0382221639156342
lr: 0.005, hidden_size: 32, layer_num: 4, train_batch_size: 4 - min val loss: 1.2338192860285442
lr: 0.005, hidden_size: 32, layer_num: 4, train_batch_size: 8 - min val loss: 1.1368884046872456
lr: 0.005, hidden_size: 32, layer_num: 4, train_batch_size: 16 - min val loss: 1.1185495058695476
lr: 0.005, hidden_size: 32, layer_num: 4, train_batch_size: 32 - min val loss: 0.9844582776228586
lr: 0.005, hidden_size: 64, layer_num: 1, train_batch_size: 4 - min val loss: 1.2411205569903057
lr: 0.005, hidden_size: 64, layer_num: 1, train_batch_size: 8 - min val loss: 1.1782208879788716
lr: 0.005, hidden_size: 64, layer_num: 1, train_batch_size: 16 - min val loss: 1.1120889981587727
lr: 0.005, hidden_size: 64, layer_num: 1, train_batch_size: 32 - min val loss: 1.032922049363454
lr: 0.005, hidden_size: 64, layer_num: 2, train_batch_size: 4 - min val loss: 1.1285174290339153
lr: 0.005, hidden_size: 64, layer_num: 2, train_batch_size: 8 - min val loss: 1.0610962808132172
lr: 0.005, hidden_size: 64, layer_num: 2, train_batch_size: 16 - min val loss: 1.0107894440491993
lr: 0.005, hidden_size: 64, layer_num: 2, train_batch_size: 32 - min val loss: 1.0000941852728527
lr: 0.005, hidden_size: 64, layer_num: 3, train_batch_size: 4 - min val loss: 1.13354488213857
lr: 0.005, hidden_size: 64, layer_num: 3, train_batch_size: 8 - min val loss: 1.0805770059426625
lr: 0.005, hidden_size: 64, layer_num: 3, train_batch_size: 16 - min val loss: 1.027336557706197
lr: 0.005, hidden_size: 64, layer_num: 3, train_batch_size: 32 - min val loss: 0.990652451912562
lr: 0.005, hidden_size: 64, layer_num: 4, train_batch_size: 4 - min val loss: 1.163758635520935
lr: 0.005, hidden_size: 64, layer_num: 4, train_batch_size: 8 - min val loss: 1.114358087380727
lr: 0.005, hidden_size: 64, layer_num: 4, train_batch_size: 16 - min val loss: 1.1012251377105713
lr: 0.005, hidden_size: 64, layer_num: 4, train_batch_size: 32 - min val loss: 1.0783658027648926
lr: 0.005, hidden_size: 128, layer_num: 1, train_batch_size: 4 - min val loss: 1.145314912001292
lr: 0.005, hidden_size: 128, layer_num: 1, train_batch_size: 8 - min val loss: 1.157290776570638
lr: 0.005, hidden_size: 128, layer_num: 1, train_batch_size: 16 - min val loss: 1.0776536067326863
lr: 0.005, hidden_size: 128, layer_num: 1, train_batch_size: 32 - min val loss: 1.0219956735769908
lr: 0.005, hidden_size: 128, layer_num: 2, train_batch_size: 4 - min val loss: 1.1322532892227173
lr: 0.005, hidden_size: 128, layer_num: 2, train_batch_size: 8 - min val loss: 1.035848339398702
lr: 0.005, hidden_size: 128, layer_num: 2, train_batch_size: 16 - min val loss: 0.983067144950231
lr: 0.005, hidden_size: 128, layer_num: 2, train_batch_size: 32 - min val loss: 0.9963303208351135
lr: 0.005, hidden_size: 128, layer_num: 3, train_batch_size: 4 - min val loss: 1.0926140149434407
lr: 0.005, hidden_size: 128, layer_num: 3, train_batch_size: 8 - min val loss: 1.0533751547336578
lr: 0.005, hidden_size: 128, layer_num: 3, train_batch_size: 16 - min val loss: 1.0611426730950673
lr: 0.005, hidden_size: 128, layer_num: 3, train_batch_size: 32 - min val loss: 0.9902568956216177
lr: 0.005, hidden_size: 128, layer_num: 4, train_batch_size: 4 - min val loss: 1.17837256193161
lr: 0.005, hidden_size: 128, layer_num: 4, train_batch_size: 8 - min val loss: 1.1211042801539104
lr: 0.005, hidden_size: 128, layer_num: 4, train_batch_size: 16 - min val loss: 1.0531088610490162
lr: 0.005, hidden_size: 128, layer_num: 4, train_batch_size: 32 - min val loss: 1.0928372542063396
lr: 0.005, hidden_size: 256, layer_num: 1, train_batch_size: 4 - min val loss: 1.1815900603930156
lr: 0.005, hidden_size: 256, layer_num: 1, train_batch_size: 8 - min val loss: 1.1558084686597188
lr: 0.005, hidden_size: 256, layer_num: 1, train_batch_size: 16 - min val loss: 1.0854611198107402
lr: 0.005, hidden_size: 256, layer_num: 1, train_batch_size: 32 - min val loss: 1.077500542004903
lr: 0.005, hidden_size: 256, layer_num: 2, train_batch_size: 4 - min val loss: 1.1073291699091594
lr: 0.005, hidden_size: 256, layer_num: 2, train_batch_size: 8 - min val loss: 1.1034573515256245
lr: 0.005, hidden_size: 256, layer_num: 2, train_batch_size: 16 - min val loss: 0.9981237749258677
lr: 0.005, hidden_size: 256, layer_num: 2, train_batch_size: 32 - min val loss: 0.9900933603445689
lr: 0.005, hidden_size: 256, layer_num: 3, train_batch_size: 4 - min val loss: 1.1148905555407207
lr: 0.005, hidden_size: 256, layer_num: 3, train_batch_size: 8 - min val loss: 1.089905281861623
lr: 0.005, hidden_size: 256, layer_num: 3, train_batch_size: 16 - min val loss: 1.0639112989107768
lr: 0.005, hidden_size: 256, layer_num: 3, train_batch_size: 32 - min val loss: 0.9738917847474416
lr: 0.005, hidden_size: 256, layer_num: 4, train_batch_size: 4 - min val loss: 1.3011542359987895
lr: 0.005, hidden_size: 256, layer_num: 4, train_batch_size: 8 - min val loss: 1.1511967778205872
lr: 0.005, hidden_size: 256, layer_num: 4, train_batch_size: 16 - min val loss: 1.0821441411972046
lr: 0.005, hidden_size: 256, layer_num: 4, train_batch_size: 32 - min val loss: 0.9836176832516988
lr: 0.005, hidden_size: 512, layer_num: 1, train_batch_size: 4 - min val loss: 1.1725616653760274
lr: 0.005, hidden_size: 512, layer_num: 1, train_batch_size: 8 - min val loss: 1.1607069373130798
lr: 0.005, hidden_size: 512, layer_num: 1, train_batch_size: 16 - min val loss: 1.1120717922846477
lr: 0.005, hidden_size: 512, layer_num: 1, train_batch_size: 32 - min val loss: 1.1408571402231853
lr: 0.005, hidden_size: 512, layer_num: 2, train_batch_size: 4 - min val loss: 1.3123668432235718
lr: 0.005, hidden_size: 512, layer_num: 2, train_batch_size: 8 - min val loss: 1.1839741071065266
lr: 0.005, hidden_size: 512, layer_num: 2, train_batch_size: 16 - min val loss: 1.0601732432842255
lr: 0.005, hidden_size: 512, layer_num: 2, train_batch_size: 32 - min val loss: 0.9952545364697775
lr: 0.005, hidden_size: 512, layer_num: 3, train_batch_size: 4 - min val loss: 1.3172320524851482
lr: 0.005, hidden_size: 512, layer_num: 3, train_batch_size: 8 - min val loss: 1.1955679059028625
lr: 0.005, hidden_size: 512, layer_num: 3, train_batch_size: 16 - min val loss: 1.0517665545145671
lr: 0.005, hidden_size: 512, layer_num: 3, train_batch_size: 32 - min val loss: 1.0432044863700867
lr: 0.005, hidden_size: 512, layer_num: 4, train_batch_size: 4 - min val loss: 1.9151435295740764
lr: 0.005, hidden_size: 512, layer_num: 4, train_batch_size: 8 - min val loss: 1.2323908805847168
lr: 0.005, hidden_size: 512, layer_num: 4, train_batch_size: 16 - min val loss: 1.1582566102345784
lr: 0.005, hidden_size: 512, layer_num: 4, train_batch_size: 32 - min val loss: 1.0777072707811992
lr: 0.0001, hidden_size: 32, layer_num: 1, train_batch_size: 4 - min val loss: 1.0255278746287029
lr: 0.0001, hidden_size: 32, layer_num: 1, train_batch_size: 8 - min val loss: 1.0435124735037486
lr: 0.0001, hidden_size: 32, layer_num: 1, train_batch_size: 16 - min val loss: 1.1327359676361084
lr: 0.0001, hidden_size: 32, layer_num: 1, train_batch_size: 32 - min val loss: 1.1711801091829936
lr: 0.0001, hidden_size: 32, layer_num: 2, train_batch_size: 4 - min val loss: 1.00819993019104
lr: 0.0001, hidden_size: 32, layer_num: 2, train_batch_size: 8 - min val loss: 1.0516598224639893
lr: 0.0001, hidden_size: 32, layer_num: 2, train_batch_size: 16 - min val loss: 1.0870506167411804
lr: 0.0001, hidden_size: 32, layer_num: 2, train_batch_size: 32 - min val loss: 1.1143388152122498
lr: 0.0001, hidden_size: 32, layer_num: 3, train_batch_size: 4 - min val loss: 1.0227807362874348
lr: 0.0001, hidden_size: 32, layer_num: 3, train_batch_size: 8 - min val loss: 1.021997610727946
lr: 0.0001, hidden_size: 32, layer_num: 3, train_batch_size: 16 - min val loss: 1.085116942723592
lr: 0.0001, hidden_size: 32, layer_num: 3, train_batch_size: 32 - min val loss: 1.098374605178833
lr: 0.0001, hidden_size: 32, layer_num: 4, train_batch_size: 4 - min val loss: 1.0430430670579274
lr: 0.0001, hidden_size: 32, layer_num: 4, train_batch_size: 8 - min val loss: 1.0669152736663818
lr: 0.0001, hidden_size: 32, layer_num: 4, train_batch_size: 16 - min val loss: 1.1105946898460388
lr: 0.0001, hidden_size: 32, layer_num: 4, train_batch_size: 32 - min val loss: 1.0967356463273366
lr: 0.0001, hidden_size: 64, layer_num: 1, train_batch_size: 4 - min val loss: 0.9470243652661642
lr: 0.0001, hidden_size: 64, layer_num: 1, train_batch_size: 8 - min val loss: 0.9643265306949615
lr: 0.0001, hidden_size: 64, layer_num: 1, train_batch_size: 16 - min val loss: 1.013512561718623
lr: 0.0001, hidden_size: 64, layer_num: 1, train_batch_size: 32 - min val loss: 1.0574089884757996
lr: 0.0001, hidden_size: 64, layer_num: 2, train_batch_size: 4 - min val loss: 0.9358924130598704
lr: 0.0001, hidden_size: 64, layer_num: 2, train_batch_size: 8 - min val loss: 1.0051679114500682
lr: 0.0001, hidden_size: 64, layer_num: 2, train_batch_size: 16 - min val loss: 1.0662026007970173
lr: 0.0001, hidden_size: 64, layer_num: 2, train_batch_size: 32 - min val loss: 1.0805332561333973
lr: 0.0001, hidden_size: 64, layer_num: 3, train_batch_size: 4 - min val loss: 0.9649568895498911
lr: 0.0001, hidden_size: 64, layer_num: 3, train_batch_size: 8 - min val loss: 0.9280304610729218
lr: 0.0001, hidden_size: 64, layer_num: 3, train_batch_size: 16 - min val loss: 0.9918935596942902
lr: 0.0001, hidden_size: 64, layer_num: 3, train_batch_size: 32 - min val loss: 1.0409312347571056
lr: 0.0001, hidden_size: 64, layer_num: 4, train_batch_size: 4 - min val loss: 0.9613461097081503
lr: 0.0001, hidden_size: 64, layer_num: 4, train_batch_size: 8 - min val loss: 0.9467311402161916
lr: 0.0001, hidden_size: 64, layer_num: 4, train_batch_size: 16 - min val loss: 1.002789964278539
lr: 0.0001, hidden_size: 64, layer_num: 4, train_batch_size: 32 - min val loss: 1.0636082490285237
lr: 0.0001, hidden_size: 128, layer_num: 1, train_batch_size: 4 - min val loss: 0.9536934395631155
lr: 0.0001, hidden_size: 128, layer_num: 1, train_batch_size: 8 - min val loss: 0.9443966845671335
lr: 0.0001, hidden_size: 128, layer_num: 1, train_batch_size: 16 - min val loss: 1.0021120607852936
lr: 0.0001, hidden_size: 128, layer_num: 1, train_batch_size: 32 - min val loss: 1.042965014775594
lr: 0.0001, hidden_size: 128, layer_num: 2, train_batch_size: 4 - min val loss: 0.8858818511168162
lr: 0.0001, hidden_size: 128, layer_num: 2, train_batch_size: 8 - min val loss: 0.9193683962027231
lr: 0.0001, hidden_size: 128, layer_num: 2, train_batch_size: 16 - min val loss: 0.9496882756551107
lr: 0.0001, hidden_size: 128, layer_num: 2, train_batch_size: 32 - min val loss: 0.9990847110748291
lr: 0.0001, hidden_size: 128, layer_num: 3, train_batch_size: 4 - min val loss: 0.8721296191215515
lr: 0.0001, hidden_size: 128, layer_num: 3, train_batch_size: 8 - min val loss: 0.9080678323904673
lr: 0.0001, hidden_size: 128, layer_num: 3, train_batch_size: 16 - min val loss: 0.9457800686359406
lr: 0.0001, hidden_size: 128, layer_num: 3, train_batch_size: 32 - min val loss: 0.9363122781117758
lr: 0.0001, hidden_size: 128, layer_num: 4, train_batch_size: 4 - min val loss: 0.8504084249337515
lr: 0.0001, hidden_size: 128, layer_num: 4, train_batch_size: 8 - min val loss: 0.9006027380625407
lr: 0.0001, hidden_size: 128, layer_num: 4, train_batch_size: 16 - min val loss: 0.9082765281200409
lr: 0.0001, hidden_size: 128, layer_num: 4, train_batch_size: 32 - min val loss: 0.9441460967063904
lr: 0.0001, hidden_size: 256, layer_num: 1, train_batch_size: 4 - min val loss: 0.9125332335631052
lr: 0.0001, hidden_size: 256, layer_num: 1, train_batch_size: 8 - min val loss: 0.9820585151513418
lr: 0.0001, hidden_size: 256, layer_num: 1, train_batch_size: 16 - min val loss: 0.9836036761601766
lr: 0.0001, hidden_size: 256, layer_num: 1, train_batch_size: 32 - min val loss: 1.007008562485377
lr: 0.0001, hidden_size: 256, layer_num: 2, train_batch_size: 4 - min val loss: 0.8401841322580973
lr: 0.0001, hidden_size: 256, layer_num: 2, train_batch_size: 8 - min val loss: 0.8858307401339213
lr: 0.0001, hidden_size: 256, layer_num: 2, train_batch_size: 16 - min val loss: 0.9172508418560028
lr: 0.0001, hidden_size: 256, layer_num: 2, train_batch_size: 32 - min val loss: 0.9622597495714823
lr: 0.0001, hidden_size: 256, layer_num: 3, train_batch_size: 4 - min val loss: 0.7764354149500529
lr: 0.0001, hidden_size: 256, layer_num: 3, train_batch_size: 8 - min val loss: 0.8312195241451263
lr: 0.0001, hidden_size: 256, layer_num: 3, train_batch_size: 16 - min val loss: 0.8793609539667765
lr: 0.0001, hidden_size: 256, layer_num: 3, train_batch_size: 32 - min val loss: 0.9348698854446411
lr: 5e-05, hidden_size: 32, layer_num: 1, train_batch_size: 4 - min val loss: 1.088671048482259
lr: 5e-05, hidden_size: 32, layer_num: 1, train_batch_size: 8 - min val loss: 1.1549005508422852
lr: 5e-05, hidden_size: 32, layer_num: 1, train_batch_size: 16 - min val loss: 1.1808813015619914
lr: 5e-05, hidden_size: 32, layer_num: 1, train_batch_size: 32 - min val loss: 1.4662357966105144
lr: 5e-05, hidden_size: 32, layer_num: 2, train_batch_size: 4 - min val loss: 1.077047308286031
lr: 5e-05, hidden_size: 32, layer_num: 2, train_batch_size: 8 - min val loss: 1.0937260786692302
lr: 5e-05, hidden_size: 32, layer_num: 2, train_batch_size: 16 - min val loss: 1.1271589001019795
lr: 5e-05, hidden_size: 32, layer_num: 2, train_batch_size: 32 - min val loss: 1.2486475904782612
