lr: 0.0001, hidden_size: 32, layer_num: 1, train_batch_size: 4 - min val loss: 1.176578164100647
lr: 0.0001, hidden_size: 32, layer_num: 1, train_batch_size: 8 - min val loss: 1.2330569624900818
lr: 0.0001, hidden_size: 32, layer_num: 1, train_batch_size: 4 - min val loss: 1.2038689653078716
lr: 0.0001, hidden_size: 32, layer_num: 1, train_batch_size: 8 - min val loss: 1.2229878703753154
lr: 0.0001, hidden_size: 32, layer_num: 1, train_batch_size: 16 - min val loss: 1.2238945960998535
lr: 0.0001, hidden_size: 32, layer_num: 1, train_batch_size: 32 - min val loss: 1.5700088540712993
lr: 0.0001, hidden_size: 32, layer_num: 2, train_batch_size: 4 - min val loss: 1.0231468677520752
lr: 0.0001, hidden_size: 32, layer_num: 2, train_batch_size: 8 - min val loss: 1.0806866983572643
lr: 0.0001, hidden_size: 32, layer_num: 2, train_batch_size: 16 - min val loss: 1.140074650446574
lr: 0.0001, hidden_size: 32, layer_num: 2, train_batch_size: 32 - min val loss: 1.2059610684712727
lr: 0.0001, hidden_size: 32, layer_num: 3, train_batch_size: 4 - min val loss: 1.017534464597702
lr: 0.0001, hidden_size: 32, layer_num: 3, train_batch_size: 8 - min val loss: 1.0459355413913727
lr: 0.0001, hidden_size: 32, layer_num: 3, train_batch_size: 16 - min val loss: 1.1654651959737141
lr: 0.0001, hidden_size: 32, layer_num: 3, train_batch_size: 32 - min val loss: 1.2787432273228962
lr: 0.0001, hidden_size: 32, layer_num: 4, train_batch_size: 4 - min val loss: 1.0217673381169636
lr: 0.0001, hidden_size: 32, layer_num: 4, train_batch_size: 8 - min val loss: 1.0811191002527873
lr: 0.0001, hidden_size: 32, layer_num: 4, train_batch_size: 16 - min val loss: 1.1302028894424438
lr: 0.0001, hidden_size: 32, layer_num: 4, train_batch_size: 32 - min val loss: 1.299981713294983
lr: 0.0001, hidden_size: 64, layer_num: 1, train_batch_size: 4 - min val loss: 1.0789069930712383
lr: 0.0001, hidden_size: 64, layer_num: 1, train_batch_size: 8 - min val loss: 1.037445992231369
lr: 0.0001, hidden_size: 64, layer_num: 1, train_batch_size: 16 - min val loss: 1.1152546207110088
lr: 0.0001, hidden_size: 64, layer_num: 1, train_batch_size: 32 - min val loss: 1.122752606868744
lr: 0.0001, hidden_size: 64, layer_num: 2, train_batch_size: 4 - min val loss: 0.9728100597858429
lr: 0.0001, hidden_size: 64, layer_num: 2, train_batch_size: 8 - min val loss: 1.0592509905497234
lr: 0.0001, hidden_size: 64, layer_num: 2, train_batch_size: 16 - min val loss: 1.10092959801356
lr: 0.0001, hidden_size: 64, layer_num: 2, train_batch_size: 32 - min val loss: 1.1162631313006084
lr: 0.0001, hidden_size: 64, layer_num: 3, train_batch_size: 4 - min val loss: 0.902634193499883
lr: 0.0001, hidden_size: 64, layer_num: 3, train_batch_size: 8 - min val loss: 1.0081005295117695
lr: 0.0001, hidden_size: 64, layer_num: 3, train_batch_size: 16 - min val loss: 1.0037008623282115
lr: 0.0001, hidden_size: 64, layer_num: 3, train_batch_size: 32 - min val loss: 1.0827669302622478
lr: 0.0001, hidden_size: 64, layer_num: 4, train_batch_size: 4 - min val loss: 0.8974397679169973
lr: 0.0001, hidden_size: 64, layer_num: 4, train_batch_size: 8 - min val loss: 0.9714090923468272
lr: 0.0001, hidden_size: 64, layer_num: 4, train_batch_size: 16 - min val loss: 0.9977987905343374
lr: 0.0001, hidden_size: 64, layer_num: 4, train_batch_size: 32 - min val loss: 1.078380564848582
lr: 0.0001, hidden_size: 128, layer_num: 1, train_batch_size: 4 - min val loss: 1.0019597907861073
lr: 0.0001, hidden_size: 128, layer_num: 1, train_batch_size: 8 - min val loss: 0.9995593031247457
lr: 0.0001, hidden_size: 128, layer_num: 1, train_batch_size: 16 - min val loss: 1.0409175058205922
lr: 0.0001, hidden_size: 128, layer_num: 1, train_batch_size: 32 - min val loss: 1.0628967086474101
lr: 0.0001, hidden_size: 128, layer_num: 2, train_batch_size: 4 - min val loss: 0.9346641898155212
lr: 0.0001, hidden_size: 128, layer_num: 2, train_batch_size: 8 - min val loss: 0.9485560754934946
lr: 0.0001, hidden_size: 128, layer_num: 2, train_batch_size: 16 - min val loss: 0.9762248595555624
lr: 0.0001, hidden_size: 128, layer_num: 2, train_batch_size: 32 - min val loss: 1.0783964296181996
lr: 0.0001, hidden_size: 128, layer_num: 3, train_batch_size: 4 - min val loss: 0.8707335491975149
lr: 0.0001, hidden_size: 128, layer_num: 3, train_batch_size: 8 - min val loss: 0.8892055551211039
lr: 0.0001, hidden_size: 128, layer_num: 3, train_batch_size: 16 - min val loss: 0.9973549544811249
lr: 0.0001, hidden_size: 128, layer_num: 3, train_batch_size: 32 - min val loss: 0.9831553300221761
lr: 0.0001, hidden_size: 128, layer_num: 4, train_batch_size: 4 - min val loss: 0.8269458909829458
lr: 0.0001, hidden_size: 128, layer_num: 4, train_batch_size: 8 - min val loss: 0.8758588135242462
lr: 0.0001, hidden_size: 128, layer_num: 4, train_batch_size: 16 - min val loss: 0.9216887851556143
lr: 0.0001, hidden_size: 128, layer_num: 4, train_batch_size: 32 - min val loss: 0.989845742781957
lr: 0.0001, hidden_size: 256, layer_num: 1, train_batch_size: 4 - min val loss: 0.9539326330025991
lr: 0.0001, hidden_size: 256, layer_num: 1, train_batch_size: 8 - min val loss: 0.9595882793267568
lr: 0.0001, hidden_size: 256, layer_num: 1, train_batch_size: 16 - min val loss: 1.0170501867930095
lr: 0.0001, hidden_size: 256, layer_num: 1, train_batch_size: 32 - min val loss: 0.9961554904778799
lr: 0.0001, hidden_size: 256, layer_num: 2, train_batch_size: 4 - min val loss: 0.8868079781532288
lr: 0.0001, hidden_size: 256, layer_num: 2, train_batch_size: 8 - min val loss: 0.895548552274704
lr: 0.0001, hidden_size: 256, layer_num: 2, train_batch_size: 16 - min val loss: 0.9571013748645782
lr: 0.0001, hidden_size: 256, layer_num: 2, train_batch_size: 32 - min val loss: 0.9749539891878763
lr: 0.0001, hidden_size: 256, layer_num: 3, train_batch_size: 4 - min val loss: 0.7922000885009766
lr: 0.0001, hidden_size: 256, layer_num: 3, train_batch_size: 8 - min val loss: 0.8526150087515513
lr: 0.0001, hidden_size: 256, layer_num: 3, train_batch_size: 16 - min val loss: 0.8547156155109406
lr: 0.0001, hidden_size: 256, layer_num: 3, train_batch_size: 32 - min val loss: 0.9592948456605276
lr: 0.0001, hidden_size: 256, layer_num: 4, train_batch_size: 4 - min val loss: 0.8207542896270752
lr: 0.0001, hidden_size: 256, layer_num: 4, train_batch_size: 8 - min val loss: 0.78483647108078
lr: 0.0001, hidden_size: 256, layer_num: 4, train_batch_size: 16 - min val loss: 0.8805615305900574
lr: 0.0001, hidden_size: 256, layer_num: 4, train_batch_size: 32 - min val loss: 0.8878088394800822
lr: 0.0001, hidden_size: 512, layer_num: 1, train_batch_size: 4 - min val loss: 0.8897638420263926
lr: 0.0001, hidden_size: 512, layer_num: 1, train_batch_size: 8 - min val loss: 0.926599899927775
lr: 0.0001, hidden_size: 512, layer_num: 1, train_batch_size: 16 - min val loss: 0.960538516441981
lr: 0.0001, hidden_size: 512, layer_num: 1, train_batch_size: 32 - min val loss: 1.0208120346069336
lr: 0.0001, hidden_size: 512, layer_num: 2, train_batch_size: 4 - min val loss: 0.8372688591480255
lr: 0.0001, hidden_size: 512, layer_num: 2, train_batch_size: 8 - min val loss: 0.8655165036519369
lr: 0.0001, hidden_size: 512, layer_num: 2, train_batch_size: 16 - min val loss: 0.8963258763154348
lr: 0.0001, hidden_size: 512, layer_num: 2, train_batch_size: 32 - min val loss: 0.9157147904237112
lr: 0.0001, hidden_size: 512, layer_num: 3, train_batch_size: 4 - min val loss: 0.7980650464693705
lr: 0.0001, hidden_size: 512, layer_num: 3, train_batch_size: 8 - min val loss: 0.8069600264231364
lr: 0.0001, hidden_size: 512, layer_num: 3, train_batch_size: 16 - min val loss: 0.7890996734301249
lr: 0.0001, hidden_size: 512, layer_num: 3, train_batch_size: 32 - min val loss: 0.8910664220650991
lr: 0.0001, hidden_size: 512, layer_num: 4, train_batch_size: 4 - min val loss: 0.8306107719739279
lr: 0.0001, hidden_size: 512, layer_num: 4, train_batch_size: 8 - min val loss: 0.8396083315213522
lr: 0.0001, hidden_size: 512, layer_num: 4, train_batch_size: 16 - min val loss: 0.8302008012930552
lr: 0.0001, hidden_size: 512, layer_num: 4, train_batch_size: 32 - min val loss: 0.8747168779373169
lr: 5e-05, hidden_size: 32, layer_num: 1, train_batch_size: 4 - min val loss: 1.2164928913116455
lr: 5e-05, hidden_size: 32, layer_num: 1, train_batch_size: 8 - min val loss: 1.276635766029358
lr: 5e-05, hidden_size: 32, layer_num: 1, train_batch_size: 16 - min val loss: 1.358443061510722
lr: 5e-05, hidden_size: 32, layer_num: 1, train_batch_size: 32 - min val loss: 2.024161159992218
lr: 5e-05, hidden_size: 32, layer_num: 2, train_batch_size: 4 - min val loss: 1.1237213214238484
lr: 5e-05, hidden_size: 32, layer_num: 2, train_batch_size: 8 - min val loss: 1.2165061036745708
lr: 5e-05, hidden_size: 32, layer_num: 2, train_batch_size: 16 - min val loss: 1.588819404443105
lr: 5e-05, hidden_size: 32, layer_num: 2, train_batch_size: 32 - min val loss: 1.8384407957394917
lr: 5e-05, hidden_size: 32, layer_num: 3, train_batch_size: 4 - min val loss: 1.1213276783625286
lr: 5e-05, hidden_size: 32, layer_num: 3, train_batch_size: 8 - min val loss: 1.1550644238789876
lr: 5e-05, hidden_size: 32, layer_num: 3, train_batch_size: 16 - min val loss: 1.2636692126592
lr: 5e-05, hidden_size: 32, layer_num: 3, train_batch_size: 32 - min val loss: 1.5116642713546753
lr: 5e-05, hidden_size: 32, layer_num: 4, train_batch_size: 4 - min val loss: 1.0828227996826172
lr: 5e-05, hidden_size: 32, layer_num: 4, train_batch_size: 8 - min val loss: 1.2005107005437214
lr: 5e-05, hidden_size: 32, layer_num: 4, train_batch_size: 16 - min val loss: 1.3449902335802715
lr: 5e-05, hidden_size: 32, layer_num: 4, train_batch_size: 32 - min val loss: 1.716532866160075
lr: 5e-05, hidden_size: 64, layer_num: 1, train_batch_size: 4 - min val loss: 1.069033940633138
lr: 5e-05, hidden_size: 64, layer_num: 1, train_batch_size: 8 - min val loss: 1.122074007987976
lr: 5e-05, hidden_size: 64, layer_num: 1, train_batch_size: 16 - min val loss: 1.1359629233678181
lr: 5e-05, hidden_size: 64, layer_num: 1, train_batch_size: 32 - min val loss: 1.3750191529591878
lr: 5e-05, hidden_size: 64, layer_num: 2, train_batch_size: 4 - min val loss: 1.0309401949246724
lr: 5e-05, hidden_size: 64, layer_num: 2, train_batch_size: 8 - min val loss: 1.0444145798683167
lr: 5e-05, hidden_size: 64, layer_num: 2, train_batch_size: 16 - min val loss: 1.1046238541603088
lr: 5e-05, hidden_size: 64, layer_num: 2, train_batch_size: 32 - min val loss: 1.2325343290964763
lr: 5e-05, hidden_size: 64, layer_num: 3, train_batch_size: 4 - min val loss: 1.0684457818667095
lr: 5e-05, hidden_size: 64, layer_num: 3, train_batch_size: 8 - min val loss: 1.019774208466212
lr: 5e-05, hidden_size: 64, layer_num: 3, train_batch_size: 16 - min val loss: 1.1125943660736084
lr: 5e-05, hidden_size: 64, layer_num: 3, train_batch_size: 32 - min val loss: 1.1119436621665955
lr: 5e-05, hidden_size: 64, layer_num: 4, train_batch_size: 4 - min val loss: 1.0359062751134236
lr: 5e-05, hidden_size: 64, layer_num: 4, train_batch_size: 8 - min val loss: 0.9940226972103119
lr: 5e-05, hidden_size: 64, layer_num: 4, train_batch_size: 16 - min val loss: 1.1016724308331807
lr: 5e-05, hidden_size: 64, layer_num: 4, train_batch_size: 32 - min val loss: 1.2064775427182515
lr: 5e-05, hidden_size: 128, layer_num: 1, train_batch_size: 4 - min val loss: 1.0239170094331105
lr: 5e-05, hidden_size: 128, layer_num: 1, train_batch_size: 8 - min val loss: 1.0334186653296153
lr: 5e-05, hidden_size: 128, layer_num: 1, train_batch_size: 16 - min val loss: 1.0698243379592896
lr: 5e-05, hidden_size: 128, layer_num: 1, train_batch_size: 32 - min val loss: 1.0622414151827495
lr: 5e-05, hidden_size: 128, layer_num: 2, train_batch_size: 4 - min val loss: 1.0312051276365917
lr: 5e-05, hidden_size: 128, layer_num: 2, train_batch_size: 8 - min val loss: 1.021525611480077
lr: 5e-05, hidden_size: 128, layer_num: 2, train_batch_size: 16 - min val loss: 1.0750399430592854
lr: 5e-05, hidden_size: 128, layer_num: 2, train_batch_size: 32 - min val loss: 1.1052579085032146
lr: 5e-05, hidden_size: 128, layer_num: 3, train_batch_size: 4 - min val loss: 0.910761167605718
lr: 5e-05, hidden_size: 128, layer_num: 3, train_batch_size: 8 - min val loss: 1.000354637702306
lr: 5e-05, hidden_size: 128, layer_num: 3, train_batch_size: 16 - min val loss: 1.0485049486160278
lr: 5e-05, hidden_size: 128, layer_num: 3, train_batch_size: 32 - min val loss: 1.0532400508721669
lr: 5e-05, hidden_size: 128, layer_num: 4, train_batch_size: 4 - min val loss: 0.8967624505360922
lr: 5e-05, hidden_size: 128, layer_num: 4, train_batch_size: 8 - min val loss: 0.9251362880071005
lr: 5e-05, hidden_size: 128, layer_num: 4, train_batch_size: 16 - min val loss: 0.9957263271013895
lr: 5e-05, hidden_size: 128, layer_num: 4, train_batch_size: 32 - min val loss: 1.0458676914374034
lr: 5e-05, hidden_size: 256, layer_num: 1, train_batch_size: 4 - min val loss: 0.9707114398479462
lr: 5e-05, hidden_size: 256, layer_num: 1, train_batch_size: 8 - min val loss: 1.0187616050243378
lr: 5e-05, hidden_size: 256, layer_num: 1, train_batch_size: 16 - min val loss: 0.9960913360118866
lr: 5e-05, hidden_size: 256, layer_num: 1, train_batch_size: 32 - min val loss: 1.021859625975291
lr: 5e-05, hidden_size: 256, layer_num: 2, train_batch_size: 4 - min val loss: 0.9659174482027689
lr: 5e-05, hidden_size: 256, layer_num: 2, train_batch_size: 8 - min val loss: 0.9723696311314901
lr: 5e-05, hidden_size: 256, layer_num: 2, train_batch_size: 16 - min val loss: 0.9909846285978953
lr: 5e-05, hidden_size: 256, layer_num: 2, train_batch_size: 32 - min val loss: 1.0642062028249104
lr: 5e-05, hidden_size: 256, layer_num: 3, train_batch_size: 4 - min val loss: 0.9010400772094727
lr: 5e-05, hidden_size: 256, layer_num: 3, train_batch_size: 8 - min val loss: 0.9158442119757334
lr: 5e-05, hidden_size: 256, layer_num: 3, train_batch_size: 16 - min val loss: 0.9257766703764597
lr: 5e-05, hidden_size: 256, layer_num: 3, train_batch_size: 32 - min val loss: 1.0639257232348125
lr: 5e-05, hidden_size: 256, layer_num: 4, train_batch_size: 4 - min val loss: 0.865720123052597
lr: 5e-05, hidden_size: 256, layer_num: 4, train_batch_size: 8 - min val loss: 0.8678732514381409
lr: 5e-05, hidden_size: 256, layer_num: 4, train_batch_size: 16 - min val loss: 0.9190455277760824
lr: 5e-05, hidden_size: 256, layer_num: 4, train_batch_size: 32 - min val loss: 0.9593834479649862
lr: 5e-05, hidden_size: 512, layer_num: 1, train_batch_size: 4 - min val loss: 0.9773035744825999
lr: 5e-05, hidden_size: 512, layer_num: 1, train_batch_size: 8 - min val loss: 0.9605462551116943
lr: 5e-05, hidden_size: 512, layer_num: 1, train_batch_size: 16 - min val loss: 0.9979525506496429
lr: 5e-05, hidden_size: 512, layer_num: 1, train_batch_size: 32 - min val loss: 1.1034035682678223
lr: 5e-05, hidden_size: 512, layer_num: 2, train_batch_size: 4 - min val loss: 0.864369660615921
lr: 5e-05, hidden_size: 512, layer_num: 2, train_batch_size: 8 - min val loss: 0.9026969969272614
lr: 5e-05, hidden_size: 512, layer_num: 2, train_batch_size: 16 - min val loss: 0.963168869415919
lr: 5e-05, hidden_size: 512, layer_num: 2, train_batch_size: 32 - min val loss: 0.9887251754601797
lr: 5e-05, hidden_size: 512, layer_num: 3, train_batch_size: 4 - min val loss: 0.8359358112017313
lr: 5e-05, hidden_size: 512, layer_num: 3, train_batch_size: 8 - min val loss: 0.872899740934372
lr: 5e-05, hidden_size: 512, layer_num: 3, train_batch_size: 16 - min val loss: 0.8922547499338785
lr: 5e-05, hidden_size: 512, layer_num: 3, train_batch_size: 32 - min val loss: 0.941808839639028
lr: 5e-05, hidden_size: 512, layer_num: 4, train_batch_size: 4 - min val loss: 0.8085979421933492
lr: 5e-05, hidden_size: 512, layer_num: 4, train_batch_size: 8 - min val loss: 0.8071756462256113
lr: 5e-05, hidden_size: 512, layer_num: 4, train_batch_size: 16 - min val loss: 0.8608551323413849
lr: 5e-05, hidden_size: 512, layer_num: 4, train_batch_size: 32 - min val loss: 0.8845051229000092
lr: 1e-05, hidden_size: 32, layer_num: 1, train_batch_size: 4 - min val loss: 2.1887599428494773
lr: 1e-05, hidden_size: 32, layer_num: 1, train_batch_size: 8 - min val loss: 2.1737887064615884
lr: 1e-05, hidden_size: 32, layer_num: 1, train_batch_size: 16 - min val loss: 2.207434892654419
lr: 1e-05, hidden_size: 32, layer_num: 1, train_batch_size: 32 - min val loss: 2.8945587078730264
lr: 1e-05, hidden_size: 32, layer_num: 2, train_batch_size: 4 - min val loss: 1.6604495644569397
lr: 1e-05, hidden_size: 32, layer_num: 2, train_batch_size: 8 - min val loss: 1.7306009729703267
lr: 1e-05, hidden_size: 32, layer_num: 2, train_batch_size: 16 - min val loss: 2.0953659613927207
lr: 1e-05, hidden_size: 32, layer_num: 2, train_batch_size: 32 - min val loss: 2.4974573850631714
lr: 1e-05, hidden_size: 32, layer_num: 3, train_batch_size: 4 - min val loss: 1.544878900051117
lr: 1e-05, hidden_size: 32, layer_num: 3, train_batch_size: 8 - min val loss: 1.8872285882631938
lr: 1e-05, hidden_size: 32, layer_num: 3, train_batch_size: 16 - min val loss: 2.0486744046211243
lr: 1e-05, hidden_size: 32, layer_num: 3, train_batch_size: 32 - min val loss: 2.4308979511260986
lr: 1e-05, hidden_size: 32, layer_num: 4, train_batch_size: 4 - min val loss: 1.6095577875773113
lr: 1e-05, hidden_size: 32, layer_num: 4, train_batch_size: 8 - min val loss: 1.8677234053611755
lr: 1e-05, hidden_size: 32, layer_num: 4, train_batch_size: 16 - min val loss: 2.095380981763204
lr: 1e-05, hidden_size: 32, layer_num: 4, train_batch_size: 32 - min val loss: 2.423932194709778
lr: 1e-05, hidden_size: 64, layer_num: 1, train_batch_size: 4 - min val loss: 1.6289350986480713
lr: 1e-05, hidden_size: 64, layer_num: 1, train_batch_size: 8 - min val loss: 1.7064114212989807
lr: 1e-05, hidden_size: 64, layer_num: 1, train_batch_size: 16 - min val loss: 2.1683669884999595
lr: 1e-05, hidden_size: 64, layer_num: 1, train_batch_size: 32 - min val loss: 2.1982215642929077
lr: 1e-05, hidden_size: 64, layer_num: 2, train_batch_size: 4 - min val loss: 1.3176360527674358
lr: 1e-05, hidden_size: 64, layer_num: 2, train_batch_size: 8 - min val loss: 1.4554635882377625
lr: 1e-05, hidden_size: 64, layer_num: 2, train_batch_size: 16 - min val loss: 1.511123239994049
lr: 1e-05, hidden_size: 64, layer_num: 2, train_batch_size: 32 - min val loss: 1.9925693273544312
lr: 1e-05, hidden_size: 64, layer_num: 3, train_batch_size: 4 - min val loss: 1.172302524248759
lr: 1e-05, hidden_size: 64, layer_num: 3, train_batch_size: 8 - min val loss: 1.2720113396644592
lr: 1e-05, hidden_size: 64, layer_num: 3, train_batch_size: 16 - min val loss: 1.464064359664917
lr: 1e-05, hidden_size: 64, layer_num: 3, train_batch_size: 32 - min val loss: 2.0446499586105347
lr: 1e-05, hidden_size: 64, layer_num: 4, train_batch_size: 4 - min val loss: 1.1803443829218547
lr: 1e-05, hidden_size: 64, layer_num: 4, train_batch_size: 8 - min val loss: 1.3828943570454915
lr: 1e-05, hidden_size: 64, layer_num: 4, train_batch_size: 16 - min val loss: 1.8632123668988545
lr: 1e-05, hidden_size: 64, layer_num: 4, train_batch_size: 32 - min val loss: 1.9824021657307942
lr: 1e-05, hidden_size: 128, layer_num: 1, train_batch_size: 4 - min val loss: 1.17671138048172
lr: 1e-05, hidden_size: 128, layer_num: 1, train_batch_size: 8 - min val loss: 1.2239713470141094
lr: 1e-05, hidden_size: 128, layer_num: 1, train_batch_size: 16 - min val loss: 1.4636961221694946
lr: 1e-05, hidden_size: 128, layer_num: 1, train_batch_size: 32 - min val loss: 1.8619369864463806
lr: 1e-05, hidden_size: 128, layer_num: 2, train_batch_size: 4 - min val loss: 1.1496831973393757
lr: 1e-05, hidden_size: 128, layer_num: 2, train_batch_size: 8 - min val loss: 1.1900207002957661
lr: 1e-05, hidden_size: 128, layer_num: 2, train_batch_size: 16 - min val loss: 1.2607149879137676
lr: 1e-05, hidden_size: 128, layer_num: 2, train_batch_size: 32 - min val loss: 1.5379118124643962
lr: 1e-05, hidden_size: 128, layer_num: 3, train_batch_size: 4 - min val loss: 1.0998093485832214
lr: 1e-05, hidden_size: 128, layer_num: 3, train_batch_size: 8 - min val loss: 1.0937565565109253
lr: 1e-05, hidden_size: 128, layer_num: 3, train_batch_size: 16 - min val loss: 1.1596086819966633
lr: 1e-05, hidden_size: 128, layer_num: 3, train_batch_size: 32 - min val loss: 1.5076584617296855
lr: 1e-05, hidden_size: 128, layer_num: 4, train_batch_size: 4 - min val loss: 1.104478398958842
lr: 1e-05, hidden_size: 128, layer_num: 4, train_batch_size: 8 - min val loss: 1.1687175234158833
lr: 1e-05, hidden_size: 128, layer_num: 4, train_batch_size: 16 - min val loss: 1.2387603720029194
lr: 1e-05, hidden_size: 128, layer_num: 4, train_batch_size: 32 - min val loss: 1.6086628437042236
lr: 1e-05, hidden_size: 256, layer_num: 1, train_batch_size: 4 - min val loss: 1.0217992564042409
lr: 1e-05, hidden_size: 256, layer_num: 1, train_batch_size: 8 - min val loss: 1.1185661554336548
lr: 1e-05, hidden_size: 256, layer_num: 1, train_batch_size: 16 - min val loss: 1.1968032717704773
lr: 1e-05, hidden_size: 256, layer_num: 1, train_batch_size: 32 - min val loss: 1.31204092502594
lr: 1e-05, hidden_size: 256, layer_num: 2, train_batch_size: 4 - min val loss: 1.1066383322079976
lr: 1e-05, hidden_size: 256, layer_num: 2, train_batch_size: 8 - min val loss: 1.0814139048258464
lr: 1e-05, hidden_size: 256, layer_num: 2, train_batch_size: 16 - min val loss: 1.1320337454477947
lr: 1e-05, hidden_size: 256, layer_num: 2, train_batch_size: 32 - min val loss: 1.1591236591339111
lr: 1e-05, hidden_size: 256, layer_num: 3, train_batch_size: 4 - min val loss: 1.06111541390419
lr: 1e-05, hidden_size: 256, layer_num: 3, train_batch_size: 8 - min val loss: 1.039099285999934
lr: 1e-05, hidden_size: 256, layer_num: 3, train_batch_size: 16 - min val loss: 1.1460884809494019
lr: 1e-05, hidden_size: 256, layer_num: 3, train_batch_size: 32 - min val loss: 1.204291085402171
lr: 1e-05, hidden_size: 256, layer_num: 4, train_batch_size: 4 - min val loss: 1.0255615413188934
lr: 1e-05, hidden_size: 256, layer_num: 4, train_batch_size: 8 - min val loss: 1.0647684534390767
lr: 1e-05, hidden_size: 256, layer_num: 4, train_batch_size: 16 - min val loss: 1.0797951022783916
lr: 1e-05, hidden_size: 256, layer_num: 4, train_batch_size: 32 - min val loss: 1.1666680375734966
lr: 1e-05, hidden_size: 512, layer_num: 1, train_batch_size: 4 - min val loss: 1.0675322314103444
lr: 1e-05, hidden_size: 512, layer_num: 1, train_batch_size: 8 - min val loss: 1.0825297832489014
lr: 1e-05, hidden_size: 512, layer_num: 1, train_batch_size: 16 - min val loss: 1.1085187792778015
lr: 1e-05, hidden_size: 512, layer_num: 1, train_batch_size: 32 - min val loss: 1.1593206922213237
lr: 1e-05, hidden_size: 512, layer_num: 2, train_batch_size: 4 - min val loss: 1.026171624660492
lr: 1e-05, hidden_size: 512, layer_num: 2, train_batch_size: 8 - min val loss: 1.099804441134135
lr: 1e-05, hidden_size: 512, layer_num: 2, train_batch_size: 16 - min val loss: 1.1138594547907512
lr: 1e-05, hidden_size: 512, layer_num: 2, train_batch_size: 32 - min val loss: 1.1543698112169902
lr: 1e-05, hidden_size: 512, layer_num: 3, train_batch_size: 4 - min val loss: 0.9639206926027933
lr: 1e-05, hidden_size: 512, layer_num: 3, train_batch_size: 8 - min val loss: 1.017766575018565
lr: 1e-05, hidden_size: 512, layer_num: 3, train_batch_size: 16 - min val loss: 1.1222610076268513
lr: 1e-05, hidden_size: 512, layer_num: 3, train_batch_size: 32 - min val loss: 1.131696105003357
lr: 1e-05, hidden_size: 512, layer_num: 4, train_batch_size: 4 - min val loss: 0.920470396677653
lr: 1e-05, hidden_size: 512, layer_num: 4, train_batch_size: 8 - min val loss: 1.000233421723048
lr: 1e-05, hidden_size: 512, layer_num: 4, train_batch_size: 16 - min val loss: 1.0596645871798198
lr: 1e-05, hidden_size: 512, layer_num: 4, train_batch_size: 32 - min val loss: 1.1573971509933472
lr: 5e-05, hidden_size: 1024, layer_num: 1, train_batch_size: 4 - min val loss: 0.9012107849121094
lr: 5e-05, hidden_size: 1024, layer_num: 1, train_batch_size: 8 - min val loss: 0.9220266044139862
lr: 5e-05, hidden_size: 1024, layer_num: 1, train_batch_size: 16 - min val loss: 1.0015461842219036
lr: 5e-05, hidden_size: 1024, layer_num: 1, train_batch_size: 32 - min val loss: 1.010332981745402
lr: 5e-05, hidden_size: 1024, layer_num: 2, train_batch_size: 4 - min val loss: 0.8210664490858713
lr: 5e-05, hidden_size: 1024, layer_num: 2, train_batch_size: 8 - min val loss: 0.8457213938236237
lr: 5e-05, hidden_size: 1024, layer_num: 2, train_batch_size: 16 - min val loss: 0.875959704319636
lr: 5e-05, hidden_size: 1024, layer_num: 2, train_batch_size: 32 - min val loss: 0.9635808169841766
lr: 5e-05, hidden_size: 1024, layer_num: 3, train_batch_size: 4 - min val loss: 0.8053085406621298
lr: 5e-05, hidden_size: 1024, layer_num: 3, train_batch_size: 8 - min val loss: 0.8259746034940084
lr: 5e-05, hidden_size: 1024, layer_num: 3, train_batch_size: 16 - min val loss: 0.8177570899327596
lr: 5e-05, hidden_size: 1024, layer_num: 3, train_batch_size: 32 - min val loss: 0.9159892102082571
lr: 5e-05, hidden_size: 1024, layer_num: 4, train_batch_size: 4 - min val loss: 0.83664271235466
lr: 5e-05, hidden_size: 1024, layer_num: 4, train_batch_size: 8 - min val loss: 0.8031459351380666
lr: 5e-05, hidden_size: 1024, layer_num: 4, train_batch_size: 16 - min val loss: 0.8109348913033804
lr: 5e-05, hidden_size: 1024, layer_num: 4, train_batch_size: 32 - min val loss: 0.8906166752179464
lr: 0.0001, hidden_size: 1024, layer_num: 1, train_batch_size: 4 - min val loss: 0.8722340563933054
lr: 0.0001, hidden_size: 1024, layer_num: 1, train_batch_size: 8 - min val loss: 0.8856289486090342
lr: 0.0001, hidden_size: 1024, layer_num: 1, train_batch_size: 16 - min val loss: 0.9307670096556345
lr: 0.0001, hidden_size: 1024, layer_num: 1, train_batch_size: 32 - min val loss: 1.003629446029663
lr: 0.0001, hidden_size: 1024, layer_num: 2, train_batch_size: 4 - min val loss: 0.7976562480131785
lr: 0.0001, hidden_size: 1024, layer_num: 2, train_batch_size: 8 - min val loss: 0.8241897225379944
lr: 0.0001, hidden_size: 1024, layer_num: 2, train_batch_size: 16 - min val loss: 0.846564362446467
lr: 0.0001, hidden_size: 1024, layer_num: 2, train_batch_size: 32 - min val loss: 0.8732162117958069
lr: 0.0001, hidden_size: 1024, layer_num: 3, train_batch_size: 4 - min val loss: 0.8433999419212341
